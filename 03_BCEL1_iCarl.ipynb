{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_BCEL1_iCarl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8AlIcK8iKdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "661a2044-0580-4ef8-bc05-5fca803209a9"
      },
      "source": [
        "pip install 'import_ipynb'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNfj-qcxiQPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ae817424-b06b-4416-af79-34611a0d6656"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "  \n",
        "%cd /content/drive/My\\ Drive/MLDLProjects/Lande_Napolitano_Pipoli/Project_root\n",
        "\n",
        "import import_ipynb\n",
        "from cifar100_Rseed import cifar_100\n",
        "from net import resnet32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/MLDLProjects/Lande_Napolitano_Pipoli/Project_root\n",
            "importing Jupyter notebook from cifar100_Rseed.ipynb\n",
            "IMPORT CIFAR DONE Rseed\n",
            "importing Jupyter notebook from net.ipynb\n",
            "IMPORT NET DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86kUTxEfVcfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4e27f3af-3198-4b52-f1ab-64ddd5d33549"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from net import resnet32\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "import random\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix as s_cm\n",
        "import seaborn as sn\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
        "\n",
        "torch.cuda.current_device()\n",
        "torch.cuda._initialized = True\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Hyper Parameters\n",
        "total_classes = 100\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvfpM-WHD5cP",
        "colab_type": "text"
      },
      "source": [
        "iCarl Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeBTHHFrffBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_on(dl, model, cif):\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  \n",
        "  # it iterates over a data loader so it returns batches of data\n",
        "  for ind, images, labels in dl: \n",
        "    labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "    images = Variable(images).cuda()\n",
        "    pred = model.classify(images, cif)#.cuda()\n",
        "    total += len(labels)\n",
        "\n",
        "    n = 0\n",
        "    for p, l in zip(pred, labels):\n",
        "      if p == l:\n",
        "        n += 1\n",
        "    correct += n\n",
        "\n",
        "  return correct / total\n",
        "\n",
        "class iCaRLNet(nn.Module):\n",
        "    def __init__(self, feature_size, n_classes, lr=2.0, gamma=0.2, weight_decay=1e-5, milestone=[49, 63], batch_size=128, num_epochs=70, momentum=0.9):\n",
        "        # Network architecture\n",
        "        super(iCaRLNet, self).__init__()\n",
        "        self.net = resnet32(num_classes=n_classes)\n",
        "        self.feature_extractor = self.net.get_fm_out\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.weight_decay = weight_decay\n",
        "        self.milestone = milestone\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.n_classes = 0\n",
        "        self.n_known = 0\n",
        "        self.momentum = momentum\n",
        "        self.exemplar_sets = []\n",
        "        self.classificationLoss = []\n",
        "        self.distillationLoss = []\n",
        "        # Learning method\n",
        "        self.MSE= nn.MSELoss() \n",
        "        self.BCEwithL = nn.BCEWithLogitsLoss()\n",
        "        self.cls_loss = nn.CrossEntropyLoss()\n",
        "      \n",
        "        # Means of exemplars\n",
        "        self.compute_means = True\n",
        "        self.exemplar_means = []\n",
        "        self.mapper = None\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      self.net = self.net.cuda()\n",
        "      return self.net.forward(x)\n",
        "\n",
        "\n",
        "    def increment_classes(self, n):\n",
        "        \"\"\"Add n classes in the final fc layer\"\"\"\n",
        "        in_features = self.fc.in_features\n",
        "        out_features = self.fc.out_features\n",
        "        weight = self.fc.weight.data\n",
        "\n",
        "        self.fc = nn.Linear(in_features, out_features + n, bias=False)\n",
        "        self.fc.weight.data[:out_features] = weight\n",
        "        self.n_classes += n\n",
        "\n",
        "    def classify(self, x, cif):\n",
        "\n",
        "        if self.compute_means:\n",
        "            print(\"Computing mean of exemplars...\")\n",
        "            exemplar_means = []\n",
        "            for P_y in self.exemplar_sets:  #P_y list of indices\n",
        "                features = np.zeros((0,64))\n",
        "                sub = Subset(cif, P_y)\n",
        "                dl = torch.utils.data.DataLoader(sub, batch_size=self.batch_size,\n",
        "                                               shuffle=False, num_workers=4)\n",
        "                # Extract feature for each exemplar in P_y\n",
        "                with torch.no_grad():\n",
        "                  for ind, ex, lab in dl:\n",
        "                    ex = Variable(ex).cuda()\n",
        "                    feature = self.feature_extractor(ex).data.cpu().numpy()\n",
        "                    feature = normalize(feature, axis=1, norm='l2')\n",
        "                    features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "                features = torch.tensor(features)\n",
        "                mu_y = features.mean(0).squeeze()\n",
        "                mu_y.data = mu_y.data / torch.norm(mu_y, p=2)  # L2 Normalize\n",
        "                exemplar_means.append(mu_y)\n",
        "\n",
        "            self.exemplar_means = exemplar_means\n",
        "            self.compute_means = False\n",
        "            print(\"Done\")\n",
        "\n",
        "\n",
        "        exemplar_means = self.exemplar_means\n",
        "        means = torch.stack(exemplar_means)  # (n_classes, feature_size)\n",
        "        means = torch.stack([means] * self.batch_size)  # (batch_size, n_classes, feature_size)\n",
        "        means = means.transpose(1, 2)  # (batch_size, feature_size, n_classes)\n",
        "\n",
        "        feature = self.feature_extractor(x)  # (batch_size, feature_size)\n",
        "        for i in range(feature.size(0)):  # Normalize\n",
        "            feature.data[i] = feature.data[i] / torch.norm(feature.data[i], p=2)\n",
        "        feature = feature.unsqueeze(2)  # (batch_size, feature_size, 1)\n",
        "        feature = feature.expand_as(means)  # (batch_size, feature_size, n_classes)\n",
        "        feature = feature.cuda()\n",
        "        means = means.cuda()\n",
        "        dists = torch.sqrt((feature - means).pow(2).sum(1)).squeeze()  # (batch_size, n_classes)\n",
        "        _, preds = dists.min(1) \n",
        "\n",
        "        return preds\n",
        "\n",
        "    def construct_exemplar_set(self, images, m):\n",
        "        \n",
        "        # Compute and cache features for each example\n",
        "        features = np.zeros((0,64))\n",
        "        indices = np.zeros((0), dtype=int)\n",
        "        dl = torch.utils.data.DataLoader(images, batch_size=self.batch_size,\n",
        "                                               shuffle=False, num_workers=4)\n",
        "        with torch.no_grad():\n",
        "          for ind, img, lab in dl:\n",
        "            x = Variable(img).cuda()\n",
        "            feature = self.feature_extractor(x).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "            indices = np.concatenate((indices,ind), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        exemplar_set = []\n",
        "        exemplar_features = np.zeros((0,64))\n",
        "\n",
        "        for k in range(1, int(m)+1):\n",
        "            S = np.sum(exemplar_features, axis=0)\n",
        "            phi = features\n",
        "            mu = class_mean\n",
        "            mu_p = 1.0 / k * (phi + S)\n",
        "            mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "            i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "            exemplar_set.append(indices[i])\n",
        "            addfeature =  np.expand_dims(features[i], axis=0)\n",
        "            exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "            #remove duplicates\n",
        "            features = np.delete(features, i, 0)\n",
        "            indices = np.delete(indices, i, 0)\n",
        "            \n",
        "        self.exemplar_sets.append(exemplar_set)\n",
        "        \n",
        "\n",
        "\n",
        "    def reduce_exemplar_sets(self, m):\n",
        "        for y, P_y in enumerate(self.exemplar_sets):\n",
        "            self.exemplar_sets[y] = P_y[:int(m)]\n",
        "\n",
        "    def combine_dataset_with_exemplars(self, cifar):\n",
        "        newindexes = []\n",
        "        for y, P_y in enumerate(self.exemplar_sets):\n",
        "            exemplar_images = P_y\n",
        "            exemplar_labels = [y] * len(P_y)\n",
        "            print(exemplar_images[0].shape)\n",
        "            newindexes+=cifar.data_append(exemplar_images, exemplar_labels)\n",
        "        return newindexes\n",
        "\n",
        "    def exemplarIndexes(self):\n",
        "      Indexes = []\n",
        "      for P_y in self.exemplar_sets:\n",
        "        Indexes += P_y\n",
        "      return Indexes\n",
        "\n",
        "    def L1_loss(self, g, q):\n",
        "      c = torch.abs(g-q)\n",
        "      d = torch.sum(c)/(g.size(0)*self.n_known)\n",
        "      return d\n",
        "\n",
        "    def update_representation(self, cifar, batchindexes, diz, file):\n",
        "        prev_model = copy.deepcopy(self)\n",
        "        prev_model = prev_model.eval().cuda()\n",
        "        self.compute_means = True\n",
        "\n",
        "        self.n_classes += 10\n",
        "\n",
        "        # Form combined training set\n",
        "        newindexes = []\n",
        "        if self.n_classes > 10:\n",
        "          newindexes = self.exemplarIndexes() #IMPORTANT!!!!\n",
        "        newindexes += list(batchindexes)\n",
        "        reprdata = Subset(cifar, newindexes)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(reprdata, batch_size=self.batch_size,\n",
        "                                             shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "        # Run network training\n",
        "\n",
        "        optimizer = optim.SGD(self.net.parameters(), lr=self.lr, weight_decay=self.weight_decay, momentum=self.momentum)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestone, gamma=self.gamma)\n",
        "        for epoch in range(self.num_epochs):\n",
        "          losses = []\n",
        "          classlosses = []\n",
        "          distlosses = []\n",
        "          for indices, images, labels in loader:\n",
        "            labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "            images = Variable(torch.FloatTensor(images)).cuda()\n",
        "            labels = Variable(labels).cuda()\n",
        "            optimizer.zero_grad()\n",
        "            g = self.forward(images)\n",
        "            y_hot = F.one_hot(labels, self.n_classes).float().cuda()\n",
        "              # cause hot are long and sigmoid is float/double\n",
        "             \n",
        "            loss = self.BCEwithL(g[:,self.n_known:self.n_classes], y_hot[:,self.n_known:self.n_classes]) \n",
        "            classlosses.append(loss.item())\n",
        "            if self.n_known > 0:\n",
        "              q = prev_model.forward(images)\n",
        "              q = torch.sigmoid(q)\n",
        "              g = torch.sigmoid(g)\n",
        "              distloss = self.L1_loss(g[:,:self.n_known],q[:,:self.n_known]) \n",
        "              distlosses.append(distloss.item())\n",
        "              loss += distloss\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "          #save data for plot\n",
        "          self.classificationLoss.append(np.mean(np.array(classlosses)))\n",
        "          if len(distlosses) == 0:\n",
        "            self.distillationLoss.append(0)\n",
        "          else:\n",
        "            self.distillationLoss.append(np.mean(np.array(distlosses)))\n",
        "\n",
        "          if (epoch + 1) % 5 == 0:\n",
        "            if len(distlosses) == 0:\n",
        "              mean_dist = 0\n",
        "            else:\n",
        "              mean_dist = np.mean(np.array(distlosses))\n",
        "            print('Epoch [%d/%d], Average_Loss(tot, dist, class): %.4f, %.4f, %.4f' \\\n",
        "                  % (epoch + 1, self.num_epochs, np.mean(np.array(losses)), mean_dist, np.mean(np.array(classlosses))), file=file)\n",
        "            print('Epoch [%d/%d], Average_Loss(tot, dist, class): %.4f, %.4f, %.4f' \\\n",
        "                  % (epoch + 1, self.num_epochs, np.mean(np.array(losses)), mean_dist, np.mean(np.array(classlosses))))\n",
        "            \n",
        "          scheduler.step()\n",
        "\n",
        "    def compute_confusion_matrix(self, cif, plot=False, title=\"Title\"):\n",
        "      \n",
        "        cifar_test = cifar_100(100, 'test')\n",
        "        dl = torch.utils.data.DataLoader(cifar_test, batch_size=128,shuffle=True, num_workers=4, drop_last=True)\n",
        "        mapper = cif.get_dictionary()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for _, images, labels in dl:\n",
        "            images = Variable(images).cuda()\n",
        "            y_true.extend(labels)\n",
        "            y_pred_tmp = self.classify(images, cif)\n",
        "            y_pred_tmp = [p.item() for p in y_pred_tmp]\n",
        "            y_pred.extend(y_pred_tmp)\n",
        "        \n",
        "        y_true = [mapper[l.item()] for l in y_true]\n",
        "        acc_matrix = s_cm(y_true, y_pred)\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(11,11))\n",
        "            df_cm = pd.DataFrame(acc_matrix)\n",
        "            sn.heatmap(df_cm, square=True, xticklabels=20, yticklabels=20)\n",
        "            plt.title(f\"{title} confusion matrix\")\n",
        "            plt.show()\n",
        "\n",
        "        return acc_matrix\n",
        "\n",
        "    def plotLosses(self, title = \"Title\"):\n",
        "      plt.rcParams['font.size'] = 22\n",
        "      x=np.arange(1, self.num_epochs*self.n_classes/10+1, 1)\n",
        "      plt.figure(figsize=(30, 15))\n",
        "      b_patch = mpatches.Patch(color='blue', label='Classification Loss')\n",
        "      r_patch = mpatches.Patch(color='red', label='Distillation Loss')\n",
        "      plt.legend(handles=[b_patch,r_patch])\n",
        "      plt.plot(x ,self.distillationLoss,'r', x, self.classificationLoss,'b')\n",
        "      plt.title(f\"{title} Loss\")\n",
        "      xT=np.arange(0, self.num_epochs*self.n_classes/10+1, self.num_epochs)\n",
        "      plt.xticks(xT)\n",
        "      plt.savefig(f'1_accStats.png')\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx4wSwCsD8TC",
        "colab_type": "text"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv4ZVuYviDK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44ae5408-6ee7-48f4-9e88-ad0fe795ffcc"
      },
      "source": [
        "# Initialize CNN\n",
        "LR = 2.0\n",
        "K = 2000  # total number of exemplars\n",
        "icarl = iCaRLNet(64, 100, num_epochs=70, lr=LR)\n",
        "\n",
        "icarl = icarl.to(DEVICE)\n",
        "BATCH_SIZE = 128\n",
        "acc_vect = []\n",
        "\n",
        "randomseed = 981\n",
        "cifarTrain = cifar_100(num_classes, 'train', randomseed)\n",
        "cifarTest = cifar_100(num_classes, 'test', randomseed)\n",
        "randomlist = cifarTrain.get_classes_list()\n",
        "diz = cifarTrain.get_dictionary()\n",
        "\n",
        "with open(\"03 - Print BCE L1.txt\", 'w') as file:\n",
        "  print(\"LR =\", LR, file=file)\n",
        "  for s in range(10):\n",
        "      print(\"Iteration %d / 10\" % s, file=file)\n",
        "      print(\"Iteration %d / 10\" % s)\n",
        "      \n",
        "      print(\"Loading training examples for classes\", randomlist[s*num_classes:s*num_classes + num_classes], file=file)\n",
        "      print(\"Loading training examples for classes\", randomlist[s*num_classes:s*num_classes + num_classes])\n",
        "      batchindexes = cifarTrain.getClassIndexes(randomlist[s*num_classes:s*num_classes + num_classes])\n",
        "      batch = Subset(cifarTrain, batchindexes)\n",
        "\n",
        "      testindexes = cifarTest.getClassIndexes(randomlist[0:s*num_classes + num_classes])\n",
        "      test_set = Subset(cifarTest, testindexes)\n",
        "\n",
        "\n",
        "      train_loader = torch.utils.data.DataLoader(batch, batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "      test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
        "                                                shuffle=False, num_workers=4, drop_last=True)\n",
        "\n",
        "      # Update representation via BackProp\n",
        "      icarl.train()\n",
        "      icarl.update_representation(cifarTrain, batchindexes, diz, file)\n",
        "      icarl.eval()\n",
        "      m = K / icarl.n_classes\n",
        "\n",
        "      # Reduce exemplar sets for known classes\n",
        "      icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "      # Construct exemplar sets for new classes\n",
        "      print(\"Constructing exemplars...\")\n",
        "      for y in randomlist[s*num_classes:s*num_classes + num_classes]:\n",
        "          imagesInd = cifarTrain.getClassIndexes([y])\n",
        "          images = Subset(cifarTrain, imagesInd)\n",
        "          # print(imagesInd)\n",
        "          icarl.construct_exemplar_set(images, m)\n",
        "      print(\"Done\")\n",
        "\n",
        "      icarl.n_known = icarl.n_classes\n",
        "\n",
        "      total = 0.0\n",
        "      correct = 0.0\n",
        "\n",
        "      print(len(train_loader))\n",
        "      for indices, images, labels in train_loader:\n",
        "          labels = [torch.tensor(diz[c.item()]) for c in labels]\n",
        "          labels = torch.tensor(labels)\n",
        "          images = Variable(images).cuda()\n",
        "          preds = icarl.classify(images, cifarTrain)\n",
        "          total = total + len(labels)\n",
        "          correct += (preds.data.cpu() == labels).sum()\n",
        "\n",
        "      print('Train Accuracy: %.2f %%' % (100.0 * correct / total), file=file)\n",
        "      print('Train Accuracy: %.2f %%' % (100.0 * correct / total))\n",
        "      total = 0.0\n",
        "      correct = 0.0\n",
        "\n",
        "      for indices, images, labels in test_loader:\n",
        "          labels = [torch.tensor(diz[c.item()]) for c in labels]\n",
        "          labels = torch.tensor(labels)\n",
        "          images = Variable(images).cuda()\n",
        "          preds = icarl.classify(images, cifarTrain)\n",
        "          total = total + len(labels)\n",
        "          correct += (preds.data.cpu() == labels).sum()\n",
        "\n",
        "      print('Test Accuracy: %.2f %%' % (100.0 * correct / total), file=file)\n",
        "      print('Test Accuracy: %.2f %%' % (100.0 * correct / total))\n",
        "      acc_vect.append(100.0 * correct.item() / total)\n",
        "      \n",
        "\n",
        "  print(acc_vect)\n",
        "  print(acc_vect, file=file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Iteration 0 / 10\n",
            "Loading training examples for classes [28, 7, 17, 45, 46, 91, 63, 13, 95, 59]\n",
            "Epoch [5/70], Average_Loss(tot, dist, class): 0.3166, 0.0000, 0.3166\n",
            "Epoch [10/70], Average_Loss(tot, dist, class): 0.2718, 0.0000, 0.2718\n",
            "Epoch [15/70], Average_Loss(tot, dist, class): 0.2536, 0.0000, 0.2536\n",
            "Epoch [20/70], Average_Loss(tot, dist, class): 0.2210, 0.0000, 0.2210\n",
            "Epoch [25/70], Average_Loss(tot, dist, class): 0.1926, 0.0000, 0.1926\n",
            "Epoch [30/70], Average_Loss(tot, dist, class): 0.1683, 0.0000, 0.1683\n",
            "Epoch [35/70], Average_Loss(tot, dist, class): 0.1363, 0.0000, 0.1363\n",
            "Epoch [40/70], Average_Loss(tot, dist, class): 0.1149, 0.0000, 0.1149\n",
            "Epoch [45/70], Average_Loss(tot, dist, class): 0.0947, 0.0000, 0.0947\n",
            "Epoch [50/70], Average_Loss(tot, dist, class): 0.0667, 0.0000, 0.0667\n",
            "Epoch [55/70], Average_Loss(tot, dist, class): 0.0446, 0.0000, 0.0446\n",
            "Epoch [60/70], Average_Loss(tot, dist, class): 0.0375, 0.0000, 0.0375\n",
            "Epoch [65/70], Average_Loss(tot, dist, class): 0.0276, 0.0000, 0.0276\n",
            "Epoch [70/70], Average_Loss(tot, dist, class): 0.0262, 0.0000, 0.0262\n",
            "Constructing exemplars...\n",
            "Done\n",
            "39\n",
            "Computing mean of exemplars...\n",
            "Done\n",
            "Train Accuracy: 96.59 %\n",
            "Test Accuracy: 83.48 %\n",
            "Iteration 1 / 10\n",
            "Loading training examples for classes [90, 69, 93, 22, 20, 39, 6, 61, 19, 81]\n",
            "Epoch [5/70], Average_Loss(tot, dist, class): 0.1700, 0.0540, 0.1160\n",
            "Epoch [10/70], Average_Loss(tot, dist, class): 0.1472, 0.0539, 0.0933\n",
            "Epoch [15/70], Average_Loss(tot, dist, class): 0.1271, 0.0511, 0.0759\n",
            "Epoch [20/70], Average_Loss(tot, dist, class): 0.1169, 0.0502, 0.0667\n",
            "Epoch [25/70], Average_Loss(tot, dist, class): 0.1063, 0.0498, 0.0564\n",
            "Epoch [30/70], Average_Loss(tot, dist, class): 0.0975, 0.0490, 0.0485\n",
            "Epoch [35/70], Average_Loss(tot, dist, class): 0.0903, 0.0482, 0.0421\n",
            "Epoch [40/70], Average_Loss(tot, dist, class): 0.0871, 0.0484, 0.0387\n",
            "Epoch [45/70], Average_Loss(tot, dist, class): 0.0852, 0.0474, 0.0377\n",
            "Epoch [50/70], Average_Loss(tot, dist, class): 0.0625, 0.0434, 0.0191\n",
            "Epoch [55/70], Average_Loss(tot, dist, class): 0.0472, 0.0387, 0.0084\n",
            "Epoch [60/70], Average_Loss(tot, dist, class): 0.0447, 0.0376, 0.0071\n",
            "Epoch [65/70], Average_Loss(tot, dist, class): 0.0407, 0.0360, 0.0046\n",
            "Epoch [70/70], Average_Loss(tot, dist, class): 0.0397, 0.0357, 0.0040\n",
            "Constructing exemplars...\n",
            "Done\n",
            "39\n",
            "Computing mean of exemplars...\n",
            "Done\n",
            "Train Accuracy: 95.65 %\n",
            "Test Accuracy: 75.68 %\n",
            "Iteration 2 / 10\n",
            "Loading training examples for classes [96, 37, 76, 40, 43, 8, 1, 83, 2, 49]\n",
            "Epoch [5/70], Average_Loss(tot, dist, class): 0.1033, 0.0377, 0.0657\n",
            "Epoch [10/70], Average_Loss(tot, dist, class): 0.0862, 0.0375, 0.0487\n",
            "Epoch [15/70], Average_Loss(tot, dist, class): 0.0787, 0.0372, 0.0415\n",
            "Epoch [20/70], Average_Loss(tot, dist, class): 0.0681, 0.0366, 0.0316\n",
            "Epoch [25/70], Average_Loss(tot, dist, class): 0.0635, 0.0353, 0.0282\n",
            "Epoch [30/70], Average_Loss(tot, dist, class): 0.0651, 0.0359, 0.0292\n",
            "Epoch [35/70], Average_Loss(tot, dist, class): 0.0559, 0.0343, 0.0216\n",
            "Epoch [40/70], Average_Loss(tot, dist, class): 0.0546, 0.0345, 0.0201\n",
            "Epoch [45/70], Average_Loss(tot, dist, class): 0.0558, 0.0342, 0.0216\n",
            "Epoch [50/70], Average_Loss(tot, dist, class): 0.0421, 0.0318, 0.0104\n",
            "Epoch [55/70], Average_Loss(tot, dist, class): 0.0321, 0.0282, 0.0039\n",
            "Epoch [60/70], Average_Loss(tot, dist, class): 0.0305, 0.0273, 0.0032\n",
            "Epoch [65/70], Average_Loss(tot, dist, class): 0.0288, 0.0264, 0.0024\n",
            "Epoch [70/70], Average_Loss(tot, dist, class): 0.0283, 0.0260, 0.0023\n",
            "Constructing exemplars...\n",
            "Done\n",
            "39\n",
            "Computing mean of exemplars...\n",
            "Done\n",
            "Train Accuracy: 94.47 %\n",
            "Test Accuracy: 71.37 %\n",
            "Iteration 3 / 10\n",
            "Loading training examples for classes [12, 27, 68, 29, 80, 86, 24, 11, 26, 51]\n",
            "Epoch [5/70], Average_Loss(tot, dist, class): 0.1066, 0.0292, 0.0774\n",
            "Epoch [10/70], Average_Loss(tot, dist, class): 0.0895, 0.0291, 0.0605\n",
            "Epoch [15/70], Average_Loss(tot, dist, class): 0.0740, 0.0290, 0.0451\n",
            "Epoch [20/70], Average_Loss(tot, dist, class): 0.0709, 0.0287, 0.0422\n",
            "Epoch [25/70], Average_Loss(tot, dist, class): 0.0627, 0.0282, 0.0345\n",
            "Epoch [30/70], Average_Loss(tot, dist, class): 0.0611, 0.0284, 0.0327\n",
            "Epoch [35/70], Average_Loss(tot, dist, class): 0.0581, 0.0280, 0.0301\n",
            "Epoch [40/70], Average_Loss(tot, dist, class): 0.0557, 0.0280, 0.0277\n",
            "Epoch [45/70], Average_Loss(tot, dist, class): 0.0496, 0.0273, 0.0223\n",
            "Epoch [50/70], Average_Loss(tot, dist, class): 0.0374, 0.0256, 0.0118\n",
            "Epoch [55/70], Average_Loss(tot, dist, class): 0.0274, 0.0235, 0.0038\n",
            "Epoch [60/70], Average_Loss(tot, dist, class): 0.0254, 0.0224, 0.0030\n",
            "Epoch [65/70], Average_Loss(tot, dist, class): 0.0241, 0.0219, 0.0023\n",
            "Epoch [70/70], Average_Loss(tot, dist, class): 0.0237, 0.0216, 0.0021\n",
            "Constructing exemplars...\n",
            "Done\n",
            "39\n",
            "Computing mean of exemplars...\n",
            "Done\n",
            "Train Accuracy: 94.09 %\n",
            "Test Accuracy: 62.80 %\n",
            "Iteration 4 / 10\n",
            "Loading training examples for classes [82, 16, 72, 52, 60, 36, 14, 41, 73, 38]\n",
            "Epoch [5/70], Average_Loss(tot, dist, class): 0.0819, 0.0222, 0.0597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhvCFrkA-UJC",
        "colab_type": "text"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYsZbjDW-TaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams['font.size'] = 16\n",
        "icarl.compute_confusion_matrix(plot=True, title = \"03 - BCE L1\", cif = cifarTrain)\n",
        "icarl.plotLosses(title =  \"03 - BCE L1\")\n",
        "plt.rcParams['font.size'] = 22\n",
        "icarl.compute_confusion_matrix(plot=True, title = \"03 - BCE L1\", cif = cifarTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCfsUuq5pXEn",
        "colab_type": "text"
      },
      "source": [
        "log 1 + x\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQcqFhewn8mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # def compute_confusion_matrix(self, cif, plot=False, title=\"Title\"):\n",
        "\n",
        "        # cifarTrain = cifar_100(num_classes, 'train', randomseed)\n",
        "        # cifarTest = cifar_100(num_classes, 'test', randomseed)\n",
        "title= \"03 - BCE L1\"\n",
        "cifarTest = cifar_100(100, 'test')\n",
        "dl = torch.utils.data.DataLoader(cifarTest, batch_size=128,shuffle=True, num_workers=4, drop_last=True)\n",
        "mapper = cifarTrain.get_dictionary()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for _, images, labels in dl:\n",
        "  images = Variable(images).cuda()\n",
        "  y_true.extend(labels)\n",
        "  y_pred_tmp = icarl.classify(images, cifarTrain)\n",
        "  y_pred_tmp = [p.item() for p in y_pred_tmp]\n",
        "  y_pred.extend(y_pred_tmp)\n",
        "\n",
        "y_true = [mapper[l.item()] for l in y_true]\n",
        "acc_matrix = s_cm(y_true, y_pred)\n",
        "acc_matrix = np.log(1+acc_matrix)\n",
        "\n",
        "plt.figure(figsize=(11,11))\n",
        "df_cm = pd.DataFrame(acc_matrix)\n",
        "sn.heatmap(df_cm, square=True, xticklabels=20, yticklabels=20)\n",
        "plt.title(f\"{title} confusion matrix\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ulAvP1amQO",
        "colab_type": "text"
      },
      "source": [
        "Acc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxNsrG8AjfhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title = \"03 - BCE L1\"\n",
        "plt.rcParams['font.size'] = 22\n",
        "x=np.arange(10, 110, 10)\n",
        "y=np.arange(0, 110, 10)\n",
        "print(x)\n",
        "plt.figure(figsize=(30, 15))\n",
        "b_patch = mpatches.Patch(color='green', label='Test Accuracy')\n",
        "plt.legend(handles=[b_patch])\n",
        "plt.plot(x ,acc_vect,'g', x ,acc_vect,'gs')\n",
        "plt.title(f\"{title} Test Accuracy\")\n",
        "plt.xticks(x)\n",
        "plt.yticks(y)\n",
        "# plt.savefig(f'1_accStats.png')\n",
        "plt.show()\n",
        "\n",
        "title =  \"03 - BCE L1\"\n",
        "plt.rcParams['font.size'] = 22\n",
        "x=np.arange(10, 110, 10)\n",
        "y=np.arange(0, 110, 10)\n",
        "print(x)\n",
        "plt.figure(figsize=(30, 15))\n",
        "b_patch = mpatches.Patch(color='green', label='Test Accuracy')\n",
        "plt.legend(handles=[b_patch])\n",
        "plt.plot(x ,acc_vect,'g')\n",
        "plt.title(f\"{title} Test Accuracy\")\n",
        "plt.xticks(x)\n",
        "plt.yticks(y)\n",
        "# plt.savefig(f'1_accStats.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}