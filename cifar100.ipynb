{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar100.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm1yvU7Itxuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class returns a list:\n",
        "# list[0] = batch of first 10 classes (0, 9) (5000 elements)\n",
        "# list[1] = batch of second 10 classes (10, 19) (5000 elements)\n",
        "# ...\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "ROOT = './data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlwJxjpEb9vH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7055ca09-8d5c-467f-fec3-42c4bb297043"
      },
      "source": [
        "# This class handles cifar-100 dataset. \n",
        "# The constructor downloads the data from torchvision.dataset repository\n",
        "# The method get_batches splits the data into batches of the specified size\n",
        "# Trainset  50k elements\n",
        "# Testset   10k elements\n",
        "\n",
        "class cifar_100(VisionDataset):\n",
        "\n",
        "    def __init__(self, classes_per_batch, split, rand_seed=None):\n",
        "        super(cifar_100, self).__init__(root=0)\n",
        "        self.__classes_per_batch = classes_per_batch\n",
        "\n",
        "        # normalize should not be used but if removed we cannot reach paper results\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__split = split\n",
        "        self.__dataset = None\n",
        "        \n",
        "        if self.__split == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "        \n",
        "        if rand_seed!=None:\n",
        "            random.seed(rand_seed)\n",
        "            self.__classes = random.sample(range(0, 100), 100)\n",
        "        else:\n",
        "            #self.__classes = np.random.permutation(100).tolist()\n",
        "            self.__classes=[36, 61, 49, 58, 92, 90, 68, 32, 28, 52, 47, 87, 1, 41, 93, 6, 88, 12, 38, 91, 81, 33, 8, 48, 60, 27, 50, 17, 56, 97, 34, 42, 84, 66, 62, 26, 29, 51, 3, 72, 39, 9, 37, 85, 13, 25, 11, 67, 99, 74, 30, 2, 64, 71, 19, 35, 31, 63, 54, 15, 43, 73, 40, 55, 7, 78, 14, 10, 70, 44, 0, 86, 79, 57, 75, 46, 83, 82, 22, 4, 45, 18, 89, 5, 59, 21, 95, 96, 69, 16, 98, 23, 80, 65, 76, 77, 20, 24, 94, 53]\n",
        "            #self.__classes=[56, 99, 35, 94, 2, 85, 96, 62, 12, 34, 19, 63, 40, 24, 79, 97, 92, 20, 7, 53, 76, 39, 17, 0, 18, 27, 74, 9, 37, 3, 45, 78, 65, 75, 16, 57, 83, 30, 1, 22, 11, 8, 38, 15, 49, 87, 26, 42, 91, 61, 6, 90, 81, 66, 44, 89, 70, 4, 23, 10, 5, 51, 14, 71, 88, 28, 41, 25, 80, 54, 55, 32, 13, 52, 36, 86, 64, 60, 93, 77, 67, 29, 21, 73, 46, 95, 68, 69, 33, 98, 58, 84, 47, 50, 72, 82, 59, 31, 48, 43]\n",
        "        \n",
        "        self.__dictionary = {}\n",
        "        self.batches = []\n",
        "\n",
        "        for i, c in enumerate(self.__classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "        self.__num_classes = 100\n",
        "        self.__indexes = []\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return index, self.dataset[index][0], self.dataset[index][1]\n",
        "\n",
        "    def get_dictionary(self):\n",
        "      return self.__dictionary\n",
        "\n",
        "    def cif_len(self):\n",
        "        return len(self.dataset.targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset.targets)\n",
        "\n",
        "    def get_tot_classes_count(self):\n",
        "        return self.__num_classes\n",
        "    \n",
        "    def get_classes_per_batch(self):\n",
        "        return self.__classes_per_batch\n",
        "\n",
        "    def getClassIndexes(self, classes):\n",
        "        indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "        labels = self.dataset.targets\n",
        "        mask = np.isin(labels, classes)\n",
        "        indexes = indexes[mask]\n",
        "        return indexes\n",
        "\n",
        "    def get_classes_list(self):\n",
        "      return self.__classes\n",
        "\n",
        "    def get_batches(self):\n",
        "\n",
        "      labels = np.array(self.dataset.targets)\n",
        "      num_elem = len(labels)\n",
        "\n",
        "      ids = [False for i in range(num_elem)]\n",
        "      for k, c in enumerate(self.__classes):\n",
        "          if k != 0 and not k % self.__classes_per_batch:\n",
        "              new_ids = []\n",
        "              for i, v in enumerate(ids):\n",
        "                if v:\n",
        "                    new_ids.append(i)\n",
        "              ids = new_ids\n",
        "              batch = Subset(self.dataset, ids)\n",
        "              self.batches.append(batch)\n",
        "\n",
        "              if k == 100:\n",
        "                break\n",
        "              ids = [False for i in range(num_elem)]\n",
        "\n",
        "          indexes = labels == c\n",
        "          ids = [i or j for i, j in zip(ids,indexes)]\n",
        "    \n",
        "      return self.batches\n",
        "\n",
        "    def getIndexesLabels(self, indexes):\n",
        "        labels=self.dataset.targets\n",
        "        lab_ind=np.array(np.arange(len(labels)))\n",
        "        mask = np.isin(lab_ind, indexes)\n",
        "        labels = np.array(labels)\n",
        "        labels = labels[mask]\n",
        "        return labels\n",
        "\n",
        "    def retrieveTrainVal(self,bat,bat_ind,cif):\n",
        "        samples = range(0,len(bat_ind))\n",
        "        #print(len(samples))\n",
        "        labels=cif.getIndexesLabels(bat_ind.tolist())\n",
        "        train, val, y_train, y_val = train_test_split(samples,labels,test_size=0.1,random_state=42,stratify=labels)\n",
        "        index_train = []\n",
        "        index_val = []\n",
        "        for i, el in enumerate(samples):\n",
        "            if el in train:\n",
        "                index_train.append(i)\n",
        "            else:\n",
        "                index_val.append(i)\n",
        "\n",
        "        train_dataset = Subset(bat, index_train)\n",
        "        val_dataset = Subset(bat, index_val)\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "    def get_transform_test(self):\n",
        "        return self.__transform_test\n",
        "\n",
        "    def get_first_k_batches(self, k):\n",
        "        k_max = self.__num_classes / self.__classes_per_batch\n",
        "        if k > k_max or k < 0:\n",
        "          print(\"ERROR: K must be a positive integer from 1 to\", int(k_max), file=sys.stderr)\n",
        "          return None\n",
        "        \n",
        "        first_k = []\n",
        "        for i in range(k):\n",
        "          first_k.extend(self.batches[i])\n",
        "        \n",
        "        return first_k\n",
        "\n",
        "    def divide_in_classes(self, batch):\n",
        "        data_in_classes = []\n",
        "        labels = [el[1] for el in batch]\n",
        "        labels = np.array(labels)\n",
        "        data = [el[0] for el in batch]\n",
        "\n",
        "        classes = np.unique(labels)\n",
        "    \n",
        "        for c in classes:\n",
        "          ids = labels == c\n",
        "        new_ids = []\n",
        "        for i, v in enumerate(ids):\n",
        "            if v:\n",
        "                new_ids.append(i)\n",
        "        ids = new_ids\n",
        "        data = Subset(data, ids)\n",
        "        data = [el for el in data]\n",
        "        data_in_classes.append(data)\n",
        "        \n",
        "        return data_in_classes\n",
        "\n",
        "print(\"IMPORT CIFAR DONE Rseed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMPORT CIFAR DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0MGrjPkq9y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}